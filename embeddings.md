# üìä Arquitectura de Embeddings - Velora Auto Evaluator

## Resumen Ejecutivo

Este documento detalla la arquitectura completa de embeddings del sistema, explicando c√≥mo funcionan, qu√© procesos los utilizan, y c√≥mo interact√∫an con la selecci√≥n de proveedores LLM.

---

## ‚ö†Ô∏è Punto Cr√≠tico: Independencia LLM vs Embeddings

### Respuesta Directa a la Pregunta

> **¬øQu√© sucede si selecciono Google Gemini como proveedor y proporciono su API Key?**

**Respuesta:** Los embeddings y el LLM son **completamente independientes**. Puedes usar cualquier combinaci√≥n:

| LLM Seleccionado | Embeddings | ¬øFunciona? |
|------------------|------------|------------|
| OpenAI GPT-4 | OpenAI Embeddings | ‚úÖ S√≠ |
| Google Gemini | OpenAI Embeddings | ‚úÖ S√≠ (si tienes OPENAI_API_KEY) |
| Google Gemini | Google Embeddings | ‚úÖ S√≠ (si tienes GOOGLE_API_KEY) |
| Anthropic Claude | OpenAI Embeddings | ‚úÖ S√≠ (si tienes OPENAI_API_KEY) |

**Importante:** Si usas Gemini como LLM pero NO tienes `OPENAI_API_KEY`, debes tener `GOOGLE_API_KEY` para que los embeddings funcionen con Google.

---

## üèóÔ∏è Arquitectura de Componentes

```mermaid
graph TD
    subgraph "UI - Streamlit"
        A[Selecci√≥n Proveedor LLM]
        B[Toggle: Semantic Matching]
        C[Toggle: LangGraph]
    end
    
    subgraph "Factories - Capa de Abstracci√≥n"
        D[LLMFactory]
        E[EmbeddingFactory]
    end
    
    subgraph "Proveedores LLM"
        F[OpenAI Chat]
        G[Google Gemini]
        H[Anthropic Claude]
    end
    
    subgraph "Proveedores Embeddings"
        I[OpenAI Embeddings]
        J[Google Embeddings]
    end
    
    subgraph "Consumidores de Embeddings"
        K[SemanticMatcher]
        L[HistoryVectorStore]
        M[HistoryChatbot RAG]
    end
    
    A --> D
    D --> F
    D --> G
    D --> H
    
    B --> E
    E --> I
    E --> J
    
    I --> K
    I --> L
    J --> K
    J --> L
    
    K --> N[Matching CV vs Requisitos]
    L --> M
    M --> O[Chatbot Historial]
```

---

## üì¶ Modelos de Embeddings Disponibles

### OpenAI (Recomendado)

| Modelo | Dimensiones | Uso Recomendado | Costo |
|--------|-------------|-----------------|-------|
| `text-embedding-3-small` | 1536 | **Default** - Balanceado | $ |
| `text-embedding-3-large` | 3072 | Alta precisi√≥n | $$$ |
| `text-embedding-ada-002` | 1536 | Anterior | $$ |

### Google

| Modelo | Dimensiones | Uso Recomendado |
|--------|-------------|-----------------|
| `models/text-embedding-004` | 768 | Default Google |
| `models/embedding-001` | 768 | Alternativa |

---

## üîÑ Flujo Completo de Embeddings

### 1. Matching Sem√°ntico CV vs Requisitos (SemanticMatcher)

```mermaid
sequenceDiagram
    participant UI as Streamlit
    participant PA as Phase1Analyzer
    participant SM as SemanticMatcher
    participant EF as EmbeddingFactory
    participant VS as FAISS VectorStore
    participant LLM as LLM (Matching)
    
    UI->>PA: analyze(job_offer, cv)
    PA->>SM: index_cv(cv_text)
    SM->>EF: create_embeddings(provider)
    EF-->>SM: embeddings instance
    SM->>VS: from_texts(chunks, embeddings)
    
    loop Para cada requisito
        PA->>SM: find_evidence(requisito)
        SM->>VS: similarity_search(requisito)
        VS-->>SM: chunks relevantes + scores
    end
    
    SM-->>PA: evidencia sem√°ntica
    PA->>LLM: match_cv_with_requirements(cv, requisitos, evidencia)
    LLM-->>PA: matches con confianza
```

**¬øCu√°ndo se usa?**
- Solo cuando el toggle "Embeddings Sem√°nticos" est√° **ACTIVADO**
- Mejora la precisi√≥n del matching al pre-filtrar informaci√≥n relevante del CV

### 2. RAG para Historial (HistoryVectorStore + HistoryChatbot)

```mermaid
sequenceDiagram
    participant UI as Streamlit - Tab Historial
    participant CB as HistoryChatbot
    participant HV as HistoryVectorStore
    participant EF as EmbeddingFactory
    participant FAISS as FAISS Index
    participant LLM as LLM (Chat)
    
    UI->>CB: query("¬øCu√°ntas veces me rechazaron?")
    CB->>HV: search(query)
    HV->>EF: create_embeddings(provider)
    HV->>FAISS: similarity_search(query)
    FAISS-->>HV: documentos relevantes
    HV-->>CB: evaluaciones relacionadas
    CB->>LLM: generate(context + query)
    LLM-->>CB: respuesta informada
    CB-->>UI: "Bas√°ndome en tu historial..."
```

**¬øCu√°ndo se usa?**
- En la pesta√±a "Mi Historial" del UI
- Permite consultas en lenguaje natural sobre evaluaciones pasadas

---

## üõ°Ô∏è Estrategia de Fallback

El sistema implementa fallback autom√°tico si el proveedor seleccionado no tiene API key:

```python
# L√≥gica en EmbeddingFactory
def get_fallback_provider() -> Optional[str]:
    """
    Retorna un proveedor de embeddings con API key disponible.
    Orden de preferencia: openai > google
    """
    for provider in ["openai", "google"]:
        if validate_api_key(provider):
            return provider
    return None
```

**Escenarios de Fallback:**

| Proveedor Solicitado | API Keys Disponibles | Resultado |
|---------------------|---------------------|-----------|
| OpenAI | OPENAI_API_KEY ‚úÖ | Usa OpenAI |
| Google | Solo OPENAI_API_KEY | Fallback a OpenAI |
| Google | GOOGLE_API_KEY ‚úÖ | Usa Google |
| OpenAI | Solo GOOGLE_API_KEY | ‚ùå Error (OpenAI requerido) |

---

## üîß Variables de Entorno

```bash
# Para LLM
OPENAI_API_KEY=sk-...        # OpenAI GPT-*
GOOGLE_API_KEY=AIza...       # Google Gemini
ANTHROPIC_API_KEY=sk-ant-... # Anthropic Claude

# Para Embeddings (usan las mismas keys)
# OpenAI Embeddings: OPENAI_API_KEY
# Google Embeddings: GOOGLE_API_KEY

# Opcional: LangSmith Tracing
LANGSMITH_API_KEY=ls-...
```

---

## üìÅ Estructura de C√≥digo

```
src/evaluator/
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # LLMFactory - Crea instancias de LLM
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_factory.py # EmbeddingFactory - Crea embeddings
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py            # Prompts para extracci√≥n y matching
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py           # Phase1Analyzer - Usa SemanticMatcher
‚îÇ   ‚îî‚îÄ‚îÄ embeddings.py         # SemanticMatcher - Matching CV vs Requisitos
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py        # HistoryVectorStore - FAISS para historial
‚îÇ   ‚îî‚îÄ‚îÄ chatbot.py            # HistoryChatbot - RAG conversacional
‚îî‚îÄ‚îÄ storage/
    ‚îî‚îÄ‚îÄ memory.py             # EnrichedEvaluation - Datos para RAG
```

---

## üéØ Casos de Uso

### Caso 1: Usuario con solo OpenAI API Key

```
Configuraci√≥n: OPENAI_API_KEY=sk-...
LLM: OpenAI GPT-4 ‚úÖ
Embeddings: OpenAI text-embedding-3-small ‚úÖ
Semantic Matching: ‚úÖ Funciona
RAG Historial: ‚úÖ Funciona
```

### Caso 2: Usuario quiere usar Gemini

```
Configuraci√≥n: GOOGLE_API_KEY=AIza...
LLM: Google Gemini ‚úÖ
Embeddings: Google text-embedding-004 ‚úÖ
Semantic Matching: ‚úÖ Funciona
RAG Historial: ‚úÖ Funciona
```

### Caso 3: Usuario con Gemini pero quiere embeddings OpenAI

```
Configuraci√≥n:
  GOOGLE_API_KEY=AIza...
  OPENAI_API_KEY=sk-...

LLM: Google Gemini ‚úÖ
Embeddings: OpenAI text-embedding-3-small ‚úÖ (Configurable)
```

### Caso 4: Usuario con Claude

```
Configuraci√≥n:
  ANTHROPIC_API_KEY=sk-ant-...
  OPENAI_API_KEY=sk-... (REQUERIDO para embeddings)

LLM: Anthropic Claude ‚úÖ
Embeddings: OpenAI text-embedding-3-small ‚úÖ
(Claude no tiene embeddings propios)
```

---

## üìä Almacenamiento de Vectores

Los vectores de embeddings se persisten en disco:

```
data/
‚îî‚îÄ‚îÄ vectors/
    ‚îî‚îÄ‚îÄ {user_id}/
        ‚îú‚îÄ‚îÄ index.faiss    # √çndice FAISS binario
        ‚îî‚îÄ‚îÄ index.pkl      # Metadata de documentos
```

**Caracter√≠sticas:**
- Un √≠ndice por usuario
- Persistencia autom√°tica tras cada evaluaci√≥n
- Recarga autom√°tica al iniciar la aplicaci√≥n
- Reconstrucci√≥n autom√°tica si se a√±aden nuevas evaluaciones

---

## ‚ö° Rendimiento

| Operaci√≥n | Tiempo T√≠pico | Notas |
|-----------|---------------|-------|
| Crear embeddings (1 chunk) | ~50ms | Depende del proveedor |
| Indexar CV (10 chunks) | ~500ms | Incluye chunking + embeddings |
| B√∫squeda sem√°ntica (1 query) | ~100ms | FAISS es muy r√°pido |
| Indexar historial (50 evals) | ~2s | Una sola vez por sesi√≥n |

---

## üîÑ Diagrama de Dependencias

```mermaid
graph LR
    subgraph "Independiente del LLM"
        A[EmbeddingFactory] --> B[SemanticMatcher]
        A --> C[HistoryVectorStore]
        C --> D[HistoryChatbot]
    end
    
    subgraph "Dependiente del LLM"
        E[LLMFactory] --> F[Phase1Analyzer]
        E --> G[Phase2Interviewer]
        E --> D
    end
    
    B --> F
    
    style A fill:#f9f,stroke:#333
    style E fill:#bbf,stroke:#333
```

---

## ‚úÖ Checklist de Configuraci√≥n

- [ ] Tengo al menos una API key de embeddings (OPENAI_API_KEY o GOOGLE_API_KEY)
- [ ] Si uso Anthropic Claude, tengo OPENAI_API_KEY para embeddings
- [ ] El directorio `data/vectors/` tiene permisos de escritura
- [ ] El toggle "Embeddings Sem√°nticos" est√° activado si quiero mejor matching

---

## üöÄ Extensibilidad

Para a√±adir un nuevo proveedor de embeddings:

1. A√±adir import condicional en `embeddings_factory.py`
2. Agregar modelos a la lista correspondiente
3. Implementar el case en `create_embeddings()`
4. Actualizar `validate_api_key()`

```python
# Ejemplo: A√±adir Cohere
try:
    from langchain_cohere import CohereEmbeddings
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False

# En create_embeddings():
elif provider_lower == "cohere":
    if not COHERE_AVAILABLE:
        raise ImportError("...")
    return CohereEmbeddings(model=embedding_model, cohere_api_key=key)
```

---

**√öltima actualizaci√≥n:** Diciembre 2024  
**Versi√≥n del documento:** 2.0

